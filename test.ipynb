{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiruu\\AppData\\Roaming\\Python\\Python39\\site-packages\\albumentations\\__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.4' (you have '2.0.3'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "Setting up [LPIPS] perceptual loss: trunk [squeeze], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=SqueezeNet1_1_Weights.IMAGENET1K_V1`. You can also use `weights=SqueezeNet1_1_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/squeezenet1_1-b8a52dc0.pth\" to C:\\Users\\tiruu/.cache\\torch\\hub\\checkpoints\\squeezenet1_1-b8a52dc0.pth\n",
      "100%|██████████| 4.73M/4.73M [00:02<00:00, 1.78MB/s]\n",
      "c:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\lpips\\weights\\v0.1\\squeeze.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                         \r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (unsigned char) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 243\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(start_epoch, end_epoch):\n\u001b[0;32m    242\u001b[0m     train_loss_G, train_loss_D, train_ssim, train_psnr, lr_G, lr_D \u001b[38;5;241m=\u001b[39m train(model, train_loader, device)\n\u001b[1;32m--> 243\u001b[0m     val_loss_G, val_loss_D, val_ssim, val_psnr \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    245\u001b[0m     model\u001b[38;5;241m.\u001b[39mstep_schedulers(val_loss_G, val_loss_D)\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;66;03m# memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)  # В мегабайтах\u001b[39;00m\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# memory_reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)   # В мегабайтах\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[1], line 50\u001b[0m, in \u001b[0;36mvalidate\u001b[1;34m(model, val_loader, device)\u001b[0m\n\u001b[0;32m     48\u001b[0m val_loss_G_history, val_loss_D_history, val_ssim_history, val_psnr_history, \u001b[38;5;241m=\u001b[39m [], [], [], []\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m real_A, real_B \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m---> 50\u001b[0m     val_loss_G, val_loss_D, val_ssim, val_psnr \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreal_B\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     val_loss_G_history\u001b[38;5;241m.\u001b[39mappend(val_loss_G)\n\u001b[0;32m     53\u001b[0m     val_loss_D_history\u001b[38;5;241m.\u001b[39mappend(val_loss_D)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\sar2opt\\models\\Pix2Pix_youtube.py:209\u001b[0m, in \u001b[0;36mPix2PixGAN.val_step\u001b[1;34m(self, real_A, real_B)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mval_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, real_A, real_B):\n\u001b[0;32m    208\u001b[0m     real_A, real_B \u001b[38;5;241m=\u001b[39m real_A\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), real_B\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 209\u001b[0m     fake_B \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_A\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    210\u001b[0m     loss_G_L1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion_L1(fake_B, real_B)\n\u001b[0;32m    211\u001b[0m     output_real \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(real_A, real_B)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\sar2opt\\models\\Pix2Pix_youtube.py:106\u001b[0m, in \u001b[0;36mGenerator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 106\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    107\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_blocks(x)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal(x)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1844\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1841\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner()\n\u001b[0;32m   1843\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1844\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1846\u001b[0m     \u001b[38;5;66;03m# run always called hooks if they have not already been run\u001b[39;00m\n\u001b[0;32m   1847\u001b[0m     \u001b[38;5;66;03m# For now only forward hooks have the always_call option but perhaps\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;66;03m# this functionality should be added to full backward hooks as well.\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m _global_forward_hooks\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1790\u001b[0m, in \u001b[0;36mModule._call_impl.<locals>.inner\u001b[1;34m()\u001b[0m\n\u001b[0;32m   1787\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m BackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[0;32m   1788\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1790\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1792\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1794\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[0;32m   1795\u001b[0m     ):\n\u001b[0;32m   1796\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tiruu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (unsigned char) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from models.Pix2Pix_youtube import Pix2PixGAN\n",
    "from utils.Dataset import *\n",
    "from utils.lossTracker import save_losses, load_losses\n",
    "import matplotlib.gridspec as gridspec\n",
    "from utils.ConfigLoader import ConfigLoader\n",
    "config = ConfigLoader()\n",
    "import torchvision\n",
    "\n",
    "import torch.profiler\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()  # Инициализация логгера\n",
    "\n",
    "def train(model, train_loader, device): \n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{end_epoch} Train\", leave=False) as pbar:\n",
    "        train_loss_G_history, train_loss_D_history, train_ssim_history, train_psnr_history = [], [], [], []\n",
    "        for real_A, real_B in pbar:\n",
    "            train_loss_G, train_loss_D, train_ssim, train_psnr = model.train_step(real_A.to(device), real_B.to(device))\n",
    "            pbar.set_postfix({\n",
    "                \"Loss D\": train_loss_D,\n",
    "                \"Loss G\": train_loss_G,\n",
    "                \"LR D\": model.optimizer_D.param_groups[0]['lr'],\n",
    "                \"LR G\": model.optimizer_G.param_groups[0]['lr'],\n",
    "            })\n",
    "            train_loss_G_history.append(train_loss_G)\n",
    "            train_loss_D_history.append(train_loss_D)\n",
    "            train_ssim_history.append(train_ssim)\n",
    "            train_psnr_history.append(train_psnr)\n",
    "    return (torch.mean(torch.tensor(train_loss_G)),\n",
    "            torch.mean(torch.tensor(train_loss_D)),\n",
    "            torch.mean(torch.tensor(train_ssim_history)),\n",
    "            torch.mean(torch.tensor(train_psnr_history)),\n",
    "            model.optimizer_G.param_groups[0]['lr'],\n",
    "            model.optimizer_D.param_groups[0]['lr'])\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model, val_loader, device):\n",
    "    \"\"\"\n",
    "    Валидационный цикл для оценки модели на валидационном датасете.\n",
    "    \"\"\"\n",
    "    model.generator.eval()  # Перевод генератора в режим валидации\n",
    "    model.discriminator.eval()  # Перевод дискриминатора в режим валидации\n",
    "\n",
    "    with tqdm(val_loader, desc=f\"Epoch {epoch+1}/{end_epoch} Validation\", leave=False) as pbar:\n",
    "        val_loss_G_history, val_loss_D_history, val_ssim_history, val_psnr_history, = [], [], [], []\n",
    "        for real_A, real_B in pbar:\n",
    "            val_loss_G, val_loss_D, val_ssim, val_psnr = model.val_step(real_A, real_B)\n",
    "\n",
    "            val_loss_G_history.append(val_loss_G)\n",
    "            val_loss_D_history.append(val_loss_D)\n",
    "            val_ssim_history.append(val_ssim)\n",
    "            val_psnr_history.append(val_psnr)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                \"Val Loss G\": val_loss_G,\n",
    "                \"Val Loss D\": val_loss_D,\n",
    "                \"Val SSIM\": val_ssim,\n",
    "                \"Val PSNR\": val_psnr\n",
    "            })\n",
    "\n",
    "    return (torch.mean(torch.tensor(val_loss_G)),\n",
    "            torch.mean(torch.tensor(val_loss_D)),\n",
    "            torch.mean(torch.tensor(val_ssim_history)),\n",
    "            torch.mean(torch.tensor(val_psnr_history)))\n",
    "\n",
    "@torch.no_grad()\n",
    "def save_training_images(model, epoch, train_loss_G, train_loss_D, val_loss_G, val_loss_D, save_dir, train_fixed_sar, train_fixed_optical, val_fixed_sar, val_fixed_optical):\n",
    "    \"\"\"\n",
    "    Сохраняет графики генератора/дискриминатора потерь и фиксированные пять изображений.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Генерация фиксированных изображений\n",
    "    train_generated = model.generator(train_fixed_sar.to(device))\n",
    "    val_generated = model.generator(val_fixed_sar.to(device))\n",
    "\n",
    "    # Move tensors to CPU for visualization and ensure same device\n",
    "    train_fixed_sar = train_fixed_sar.cpu().repeat(1, 3, 1, 1)\n",
    "    train_generated = train_generated.cpu()\n",
    "    train_fixed_optical = train_fixed_optical.cpu()\n",
    "    val_fixed_sar = val_fixed_sar.cpu().repeat(1, 3, 1, 1)\n",
    "    val_generated = val_generated.cpu()\n",
    "    val_fixed_optical = val_fixed_optical.cpu()\n",
    "\n",
    "    # Log generated images to TensorBoard\n",
    "    train_grid = torchvision.utils.make_grid(\n",
    "        torch.cat([\n",
    "            train_fixed_sar, \n",
    "            train_generated,\n",
    "            train_fixed_optical\n",
    "        ], dim=0),\n",
    "        nrow=5,\n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    val_grid = torchvision.utils.make_grid(\n",
    "        torch.cat([\n",
    "            val_fixed_sar,\n",
    "            val_generated, \n",
    "            val_fixed_optical\n",
    "        ], dim=0),\n",
    "        nrow=5, \n",
    "        normalize=True\n",
    "    )\n",
    "\n",
    "    fig = plt.figure(figsize=(30, 40))\n",
    "    gs = gridspec.GridSpec(7, 6, figure=fig)\n",
    "\n",
    "    fig.suptitle(f\"Epoch: {epoch+1}, G lr: {model.optimizer_D.param_groups[0]['lr']}, D lr: {model.optimizer_G.param_groups[0]['lr']}\", fontsize=16)  # y задает отступ сверху\n",
    "\n",
    "    # График потерь генератора\n",
    "    ax1 = fig.add_subplot(gs[0, :3])\n",
    "    ax1.plot(range(1, len(train_loss_G) + 1), train_loss_G, label=\"Train Generator Loss\", color=\"#3b82f6\")\n",
    "    ax1.set_title(\"Train Generator Loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.grid()\n",
    "    ax1.legend()\n",
    "\n",
    "    # График потерь дискриминатора\n",
    "    ax2 = fig.add_subplot(gs[1, :3])\n",
    "    ax2.plot(range(1, len(train_loss_D) + 1), train_loss_D, label=\"Train Discriminator Loss\", color=\"#ef4444\")\n",
    "    ax2.set_title(\"Train Discriminator Loss\")\n",
    "    ax2.set_xlabel(\"Epochs\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.grid()\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3 = fig.add_subplot(gs[0, 3:])\n",
    "    ax3.plot(range(1, len(val_loss_G) + 1), val_loss_G, label=\"Val Generator Loss\", color=\"#22c55e\")\n",
    "    ax3.set_title(\"Validation Generator Loss\")\n",
    "    ax3.set_xlabel(\"Epochs\")\n",
    "    ax3.set_ylabel(\"Loss\")\n",
    "    ax3.grid()\n",
    "    ax3.legend()\n",
    "\n",
    "    ax4 = fig.add_subplot(gs[1, 3:])\n",
    "    ax4.plot(range(1, len(val_loss_D) + 1), val_loss_D, label=\"Val Discriminator Loss\", color=\"#f59e0b\")\n",
    "    ax4.set_title(\"Validation Discriminator Loss\")\n",
    "    ax4.set_xlabel(\"Epochs\")\n",
    "    ax4.set_ylabel(\"Loss\")\n",
    "    ax4.grid()\n",
    "    ax4.legend()\n",
    "\n",
    "    # Добавляем изображения: SAR, Generated, Target\n",
    "    for i in range(5):\n",
    "        # SAR Image\n",
    "        ax_sar = fig.add_subplot(gs[2 + i, 0])\n",
    "        sar_image = train_fixed_sar[i, 0, :, :].cpu().detach().numpy()\n",
    "        ax_sar.imshow(sar_image * 0.5 + 0.5, cmap='gray')\n",
    "        ax_sar.set_title(f\"Train SAR Image {i+1}\")\n",
    "        ax_sar.axis('off')\n",
    "\n",
    "        # Generated Image\n",
    "        ax_gen = fig.add_subplot(gs[2 + i, 1])\n",
    "        generated_image = train_generated[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        ax_gen.imshow((generated_image * 0.5 + 0.5))\n",
    "        ax_gen.set_title(f\"Train Generated Image {i+1}\")\n",
    "        ax_gen.axis('off')\n",
    "\n",
    "        # Target Image\n",
    "        ax_opt = fig.add_subplot(gs[2 + i, 2])\n",
    "        optical_image = train_fixed_optical[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        ax_opt.imshow((optical_image * 0.5 + 0.5))\n",
    "        ax_opt.set_title(f\"Train Target Image {i+1}\")\n",
    "        ax_opt.axis('off')\n",
    "\n",
    "        # SAR Image\n",
    "        ax_sar = fig.add_subplot(gs[2 + i, 3])\n",
    "        sar_image = val_fixed_sar[i, 0, :, :].cpu().detach().numpy()\n",
    "        ax_sar.imshow(sar_image * 0.5 + 0.5, cmap='gray')\n",
    "        ax_sar.set_title(f\"Val SAR Image {i+1}\")\n",
    "        ax_sar.axis('off')\n",
    "\n",
    "        # Generated Image\n",
    "        ax_gen = fig.add_subplot(gs[2 + i, 4])\n",
    "        generated_image = val_generated[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        ax_gen.imshow((generated_image * 0.5 + 0.5))\n",
    "        ax_gen.set_title(f\"Val Generated Image {i+1}\")\n",
    "        ax_gen.axis('off')\n",
    "\n",
    "        # Target Image\n",
    "        ax_opt = fig.add_subplot(gs[2 + i, 5])\n",
    "        optical_image = val_fixed_optical[i].permute(1, 2, 0).cpu().detach().numpy()\n",
    "        ax_opt.imshow((optical_image * 0.5 + 0.5))\n",
    "        ax_opt.set_title(f\"Val Target Image {i+1}\")\n",
    "        ax_opt.axis('off')\n",
    "\n",
    "    # Настройка расстояний между элементами\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Сохранение итогового изображения\n",
    "    save_path = os.path.join(save_dir, f\"epoch_{epoch+1}_images.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close(fig)\n",
    "    return train_grid, val_grid\n",
    "\n",
    "\n",
    "# Устройство для вычислений\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.mps.is_available() else \"cpu\")\n",
    "print(f'Using {device}')\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Получаем 5 первых изображений из тренировочного загрузчика\n",
    "train_iterator = iter(train_loader)  # Создаем итератор для train_loader\n",
    "fixed_batch = next(train_iterator)  # Получаем первый batch\n",
    "train_fixed_sar, train_fixed_optical = fixed_batch[0][:5], fixed_batch[1][:5]  # Берем 5 первых изображений\n",
    "\n",
    "test_iterator = iter(test_loader)  # Создаем итератор для train_loader\n",
    "fixed_batch = next(test_iterator)  # Получаем первый batch\n",
    "val_fixed_sar, val_fixed_optical = fixed_batch[0][:5], fixed_batch[1][:5]  # Берем 5 первых изображений\n",
    "\n",
    "# Создание модели\n",
    "model = Pix2PixGAN(device)\n",
    "\n",
    "# Загрузка модели\n",
    "if config.get('model', 'load_model'):\n",
    "    start_epoch = model.load_state('checkpoint_epoch_100', device)\n",
    "    losses_dict = load_losses()\n",
    "    if losses_dict:\n",
    "        train_G_losses = list(losses_dict['train_G_losses'])\n",
    "        train_D_losses = list(losses_dict['train_D_losses'])\n",
    "        val_G_losses = list(losses_dict['val_G_losses'])\n",
    "        val_D_losses = list(losses_dict['val_D_losses'])\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    train_G_losses = []\n",
    "    train_D_losses = []\n",
    "    val_G_losses = []\n",
    "    val_D_losses = []\n",
    "\n",
    "# Конечная эпоха\n",
    "end_epoch = config.get('model', 'end_epoch')\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "# Обучение модели\n",
    "for epoch in range(start_epoch, end_epoch):\n",
    "    train_loss_G, train_loss_D, train_ssim, train_psnr, lr_G, lr_D = train(model, train_loader, device)\n",
    "    val_loss_G, val_loss_D, val_ssim, val_psnr = validate(model, test_loader, device)\n",
    "\n",
    "    model.step_schedulers(val_loss_G, val_loss_D)\n",
    "\n",
    "    # memory_allocated = torch.cuda.memory_allocated(device) / (1024 ** 2)  # В мегабайтах\n",
    "    # memory_reserved = torch.cuda.memory_reserved(device) / (1024 ** 2)   # В мегабайтах\n",
    "\n",
    "    train_G_losses.append(train_loss_G)\n",
    "    train_D_losses.append(train_loss_D)\n",
    "    val_G_losses.append(val_loss_G)\n",
    "    val_D_losses.append(val_loss_D)\n",
    "\n",
    "    if writer:\n",
    "        writer.add_scalar(\"Train/Loss_G\", train_loss_G.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Train/Loss_D\", train_loss_D.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Train/PSNR\", train_psnr.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Train/SSIM\", train_ssim.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Train/Learning_Rate_G\", lr_G, epoch + 1)\n",
    "        writer.add_scalar(\"Train/Learning_Rate_D\", lr_D, epoch + 1)\n",
    "\n",
    "        writer.add_scalar(\"Val/Loss_G\", val_loss_G.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Val/Loss_D\", val_loss_D.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Val/PSNR\", val_psnr.item(), epoch + 1)\n",
    "        writer.add_scalar(\"Val/SSIM\", val_ssim.item(), epoch + 1)\n",
    "\n",
    "        # Гистограммы весов\n",
    "        # for name, param in model.generator.named_parameters():\n",
    "        #     writer.add_histogram(f'Generator/{name}', param, epoch + 1)\n",
    "\n",
    "        # Логгирование памяти в TensorBoard\n",
    "        # writer.add_scalar(\"Performance/Memory_Allocated_MB\", memory_allocated, global_step=epoch)\n",
    "        # writer.add_scalar(\"Performance/Memory_Reserved_MB\", memory_reserved, global_step=epoch)\n",
    "\n",
    "    # Сохранение модели и метрик\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        # model.save_state(epoch, save_dir=config.get('paths', 'model_save_dir'))\n",
    "        save_losses(\n",
    "            train_G_losses=train_G_losses,\n",
    "            train_D_losses=train_D_losses,\n",
    "            val_G_losses=val_G_losses,\n",
    "            val_D_losses=val_D_losses\n",
    "        )\n",
    "\n",
    "    # Сохранение изображений каждые 20 эпох\n",
    "    if (epoch + 1) % 20 == 0:\n",
    "        train_grid, val_grid = save_training_images(\n",
    "            model,\n",
    "            epoch,\n",
    "            train_G_losses,\n",
    "            train_D_losses,\n",
    "            val_G_losses,\n",
    "            val_D_losses,\n",
    "            config.get('paths', 'image_save_dir'),\n",
    "            train_fixed_sar, train_fixed_optical,\n",
    "            val_fixed_sar, val_fixed_optical\n",
    "        )\n",
    "        writer.add_image('Train/Train_Images', train_grid, global_step=epoch+1)\n",
    "        writer.add_image('Val/Val_Images', val_grid, global_step=epoch+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "os.system(\"shutdown /s /t 60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
